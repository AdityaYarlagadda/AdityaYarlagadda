{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled50.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCmaPwa_svkb"
      },
      "source": [
        "# Imoprting the libraries.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import layers"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuYY-XCrtBH6",
        "outputId": "a4b09175-471e-4c88-e625-bce5a283bcbd"
      },
      "source": [
        "#Loading the data set\n",
        "data_set = pd.read_csv('imdb_master.csv', encoding='latin-1')\n",
        "#Dispplaying the head of the data set.\n",
        "print(data_set.head())\n",
        "#Review the dataset values.\n",
        "sentences = data_set['review'].values\n",
        "y = data_set['label'].values"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  type  ... label         file\n",
            "0           0  test  ...   neg      0_2.txt\n",
            "1           1  test  ...   neg  10000_4.txt\n",
            "2           2  test  ...   neg  10001_1.txt\n",
            "3           3  test  ...   neg  10002_3.txt\n",
            "4           4  test  ...   neg  10003_3.txt\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjFQ6hjxtEe-"
      },
      "source": [
        "#tokenizing the dataset.\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRQ1UKHBtPYq"
      },
      "source": [
        "#The tokenizer class is texts to matrix function for converting the document into a numpy matrix form.\n",
        "sentences = tokenizer.texts_to_matrix(sentences)\n",
        "#The LabelEncoder is a utility class which to help out the normalize labels such that they contains a only values between 0 and classes-1.\n",
        "le = preprocessing.LabelEncoder()\n",
        "# Below we have used the Fit transform method on the input data at a single time and converts the data points.\n",
        "y = le.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n",
        "inptdim=np.prod(X_train.shape[1:]) #input the dim"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIXU2nrttR_R",
        "outputId": "7efcbdbc-c7a8-496b-e328-093ad98ca9be"
      },
      "source": [
        "# Modeling and adding the layers below.\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(300, input_dim= inptdim, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax')) \n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "history=model.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)\n",
        "# Evaluating model test data below.\n",
        "[test_loss, test_acc] = model.evaluate(X_test,y_test)\n",
        "print(\"The evaluation of the result on the Test Data set : Loss = {}, accuracy = {}\".format(test_loss, test_acc))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "293/293 [==============================] - 3s 8ms/step - loss: 0.8803 - acc: 0.5044 - val_loss: 0.8426 - val_acc: 0.5146\n",
            "Epoch 2/5\n",
            "293/293 [==============================] - 2s 7ms/step - loss: 0.7903 - acc: 0.5784 - val_loss: 0.8419 - val_acc: 0.5142\n",
            "Epoch 3/5\n",
            "293/293 [==============================] - 2s 6ms/step - loss: 0.6827 - acc: 0.6833 - val_loss: 0.8722 - val_acc: 0.5148\n",
            "Epoch 4/5\n",
            "293/293 [==============================] - 2s 6ms/step - loss: 0.4867 - acc: 0.8205 - val_loss: 0.9548 - val_acc: 0.5056\n",
            "Epoch 5/5\n",
            "293/293 [==============================] - 2s 7ms/step - loss: 0.2773 - acc: 0.9297 - val_loss: 1.0676 - val_acc: 0.5050\n",
            "782/782 [==============================] - 3s 3ms/step - loss: 1.0676 - acc: 0.5050\n",
            "The evaluation of the result on the Test Data set : Loss = 1.067567229270935, accuracy = 0.5050399899482727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6OFb0f6thvE"
      },
      "source": [
        "# Adding the Embedding layer to the model.\n",
        "sentences_test = data_set['review']\n",
        "#Spliting the data set \n",
        "max_review_len= max([len(s.split()) for s in sentences_test])\n",
        "vocab_size= len(tokenizer.word_index)+1\n",
        "sentences_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "padded= pad_sequences(sentences_test,maxlen=max_review_len)\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.25, random_state=1000)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_XU35xPtmc1",
        "outputId": "41a6ad18-dd1c-4c6c-b25f-af414a745915"
      },
      "source": [
        "#adding the dense layers below.\n",
        "models = Sequential()\n",
        "models.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
        "models.add(Flatten())\n",
        "models.add(layers.Dense(300, activation='relu',input_dim=max_review_len))\n",
        "models.add(layers.Dense(3, activation='softmax'))\n",
        "models.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "history1=models.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "293/293 [==============================] - 28s 94ms/step - loss: 1.0208 - acc: 0.4897 - val_loss: 0.8701 - val_acc: 0.4986\n",
            "Epoch 2/5\n",
            "293/293 [==============================] - 27s 93ms/step - loss: 0.8248 - acc: 0.5371 - val_loss: 0.8461 - val_acc: 0.5016\n",
            "Epoch 3/5\n",
            "293/293 [==============================] - 28s 94ms/step - loss: 0.7387 - acc: 0.6140 - val_loss: 0.8856 - val_acc: 0.4972\n",
            "Epoch 4/5\n",
            "293/293 [==============================] - 27s 93ms/step - loss: 0.6164 - acc: 0.7049 - val_loss: 0.9893 - val_acc: 0.4881\n",
            "Epoch 5/5\n",
            "293/293 [==============================] - 27s 92ms/step - loss: 0.4798 - acc: 0.7889 - val_loss: 1.1556 - val_acc: 0.4830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neg_3cPttrJu"
      },
      "source": [
        "# Applying the same for 20news data\n",
        "# Importing the fetch_20newsgroups from the library\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "sentences_train = newsgroups_train.data\n",
        "y = newsgroups_train.target\n",
        "# Spliting the train data set below.\n",
        "max_review_len= max([len(s.split()) for s in sentences_train])\n",
        "vocab_size= len(tokenizer.word_index)+1\n",
        "# Using the Tests_to_sequences mentod below.\n",
        "sentences_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "padded= pad_sequences(sentences_train,maxlen=max_review_len)\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.25, random_state=1000)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjJ5BBKf7a04",
        "outputId": "b1d301c3-75f1-467c-aea8-184d03d7e5b8"
      },
      "source": [
        "#MOdeling and adding the dense layers.\n",
        "model_se = Sequential()\n",
        "model_se.add(Embedding(vocab_size, 50, input_length=max_review_len))\n",
        "model_se.add(Flatten())\n",
        "model_se.add(layers.Dense(300, activation='relu',input_dim=max_review_len))\n",
        "model_se.add(layers.Dense(20, activation='softmax'))\n",
        "#compile the model.\n",
        "model_se.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "#Fit the train and validation of test data set.\n",
        "history2=model_se.fit(X_train,y_train, epochs=5, verbose=True, validation_data=(X_test,y_test), batch_size=256)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "34/34 [==============================] - 16s 451ms/step - loss: 9.6063 - acc: 0.0533 - val_loss: 3.0079 - val_acc: 0.0583\n",
            "Epoch 2/5\n",
            "34/34 [==============================] - 15s 440ms/step - loss: 2.9956 - acc: 0.0537 - val_loss: 2.9913 - val_acc: 0.0562\n",
            "Epoch 3/5\n",
            "34/34 [==============================] - 15s 437ms/step - loss: 2.9853 - acc: 0.0536 - val_loss: 2.9908 - val_acc: 0.0530\n",
            "Epoch 4/5\n",
            "34/34 [==============================] - 15s 436ms/step - loss: 2.9870 - acc: 0.0554 - val_loss: 2.9905 - val_acc: 0.0537\n",
            "Epoch 5/5\n",
            "34/34 [==============================] - 15s 436ms/step - loss: 2.9815 - acc: 0.0583 - val_loss: 2.9905 - val_acc: 0.0527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUQvYUXptvLW",
        "outputId": "4e92bf90-406c-4e91-f0fb-4ff5fe5c1a8e"
      },
      "source": [
        "# predicting over the sample data\n",
        "print('The Actual Values are ',y_test[4],'The Predicted Values are ',model_se.predict(X_test[[4],:]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Actual Values are  4 The Predicted Values are  [[0.04887531 0.05042714 0.04998025 0.05006889 0.05019574 0.05016339\n",
            "  0.05027804 0.04998903 0.05002546 0.05051598 0.05010793 0.04986186\n",
            "  0.05032712 0.05027978 0.05019875 0.05095404 0.04979479 0.05037596\n",
            "  0.0492017  0.04837884]]\n"
          ]
        }
      ]
    }
  ]
}